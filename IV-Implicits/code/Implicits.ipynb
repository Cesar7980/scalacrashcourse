{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scala implicits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implicits in Scala are also pretty ubiquous. For instance, we can find them in the Scala standard library, and two popular frameworks: akka stream and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futureapi](../misc/futureapi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futureapply](../misc/futureapply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futurefoldleft](../misc/futurefoldleft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![akka-flow](../misc/akka-api-flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![akka-flow-runwith](../misc/akka-api-flow-runwith.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-dataset](../misc/spark-api-dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dataset-flatmap](../misc/spark-api-dataset-flatmap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![spark-groupbykey](../misc/spark-api-dataset-groupbykey.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits as term inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw that the Scala compiler can infer the type parameters that have to be applied in a call to a generic function. For instance, given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo[T](i: T): T = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we may call our function passing explicitly the type parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo[Int](1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or we may omit the type parameter and let the Scala compiler to infer its proper value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can values also be inferred by the Scala compiler? Yes, they can! For instance, in order to create an asynchronous computation using Scala `Future`s, we need to pass its `ExecutionContext`, i.e. the component that will be in charge of executing it. This is the signature of the factory method: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![futureapply](../misc/futureapply.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the execution context parameter is maked as `implicit` which means that the Scala compiler, in principle, can infer the value of that parameter for us. Of coruse, we can also pass the parameter explicitly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scala.concurrent.{Future, ExecutionContext}\n",
    "\n",
    "Future[Int](1)(ExecutionContext.global)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but the idiomatic way to write this invocation is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Future(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops. We forgot an important detail. In order for Scala to infer the type parameter we didn't have to do anything: Scala infers the type `Int` just from the type of the code to be executed. However, in order to infer the execution context, Scala needs some help in the form of `implicit` declarations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "implicit val ec: ExecutionContext = ExecutionContext.global\n",
    "\n",
    "Future(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In sum, if we want Scala to infer the value of some parameter we mark it as `implicit`. If we want some value to be used as the implicit parameter for a given type, we declare that value as `implicit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits for context parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous example shows a very common use case of implicit parameters: to inject runtime dependencies, i.e. contextual parameters that are needed in order to execute our code. This is the case with the `ExecutionContext` for `Future`-based computations, or the `Materializer` component that we need to run a streaming pipeline of Akka stream. Let's analyse this later example with some more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                     \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.time._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.concurrent._, duration._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36makka._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36makka.actor._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36makka.stream._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36makka.stream.scaladsl._\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`com.typesafe.akka::akka-stream:2.6.4`\n",
    "import java.time._\n",
    "import scala.concurrent._, duration._\n",
    "import akka._\n",
    "import akka.actor._\n",
    "import akka.stream._\n",
    "import akka.stream.scaladsl._\n",
    "\n",
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that we have this pipeline that simply generates a stream of integer values which are then printed in the console:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd5.sc:8: materializing requested scala.reflect.type.ClassTag[akka.stream.scaladsl.RunnableGraph[akka.NotUsed]] using scala.reflect.`package`.materializeClassTag[akka.stream.scaladsl.RunnableGraph[akka.NotUsed]]()\n",
      "  .printOnChange(pipeline, \"pipeline\", _root_.scala.None, _root_.scala.None, _root_.scala.None)) }\n",
      "                ^"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpipeline\u001b[39m: \u001b[32mRunnableGraph\u001b[39m[\u001b[32mNotUsed\u001b[39m] = \u001b[33mRunnableGraph\u001b[39m(\n",
       "  \u001b[33mLinearTraversalBuilder\u001b[39m(\n",
       "    None,\n",
       "    None,\n",
       "..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline: RunnableGraph[NotUsed] = \n",
    "    Source(List(1,2,3,4)).to(Sink.foreach(println))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just a declarative program. If we want our pipeline to be executed we need to `run` it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd6.sc:8: materializing requested scala.reflect.type.ClassTag[akka.NotUsed] using scala.reflect.`package`.materializeClassTag[akka.NotUsed]()\n",
      "          .print(res6, \"res6\", _root_.scala.None)\n",
      "                ^"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres6\u001b[39m: \u001b[32mNotUsed\u001b[39m = NotUsed"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oooops (again). In order to run a pipeline we need a `Materializer`, as the `run` signature shows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![run](../misc/runnablegraph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![runnablegraphrun](../misc/runnablegraphrun.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard `Materializer` that is provided by the akka stream library executes pipelines by means of an actor system. So, in order to declare the implicit `Materializer` value, we first need to create an `ActorSystem`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msystem\u001b[39m: \u001b[32mActorSystem\u001b[39m = akka://akka-stream-primer"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit val system: ActorSystem = ActorSystem(\"akka-stream-primer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can now run our pipeline: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd7.sc:8: materializing requested scala.reflect.type.ClassTag[akka.NotUsed] using scala.reflect.`package`.materializeClassTag[akka.NotUsed]()\n",
      "          .print(res7, \"res7\", _root_.scala.None)\n",
      "                ^1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mres7\u001b[39m: \u001b[32mNotUsed\u001b[39m = NotUsed"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, wait, Scala needs an implicit value of the type `Materializer`, and we defined an implicit value of type `ActorSystem`. How did Scala infer an implicit value for the former type? The answer lies in the following declaration located in the companion object of the `Materializer` class: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "\n",
    "object Materializer {\n",
    "\n",
    "  implicit def matFromSystem(\n",
    "      implicit provider: ClassicActorSystemProvider): Materializer =\n",
    "    ???\n",
    "\n",
    "}\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`matFromSystem` is called a _conditional implicit value_, meaning that Scala can construct an implicit value for the `Materializer` type, _provided that_ it can infer an implicit value for a `ClassicActorSystemProvider`. In our example, it certainly can, since we declare an implicit value of the type `ActorSystem` (which extends the trait `ClassicActorSystemProvider`). Ok, but how did the Scala compiler know that there were such a conditional implicit value? Because it was not certainly in scope. Right, but `matFromSystem` is defined in the _companion object_ of the `Materializer` type, and Scala looks automatically for implicits there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.configureCompiler(_.settings.XlogImplicits.value = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.configureCompiler(_.settings.logImplicitConv.value = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp.configureCompiler(_.settings.warnExtraImplicit.value = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Implicits for class extension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common use case for implicits is the implementation of extension methods: there is a class for which we have no control, or no interest in modifying its definition, and, yet, we would like to add some new methods. The Scala standard library of Scala has good examples of this pattern. For instance, the `String` class comes actually from Java:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36ms\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"hi\"\u001b[39m\n",
       "\u001b[36mres18_1\u001b[39m: \u001b[32mClass\u001b[39m[\u001b[32mT\u001b[39m] = class java.lang.String"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val s: String = \"hi\"\n",
    "s.getClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and `java.lang.String` does not declares a `map` method. How then can we make this call?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres19\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"HI\"\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.map(c => c.toUpper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer lies in the following declaration in the `Predef.scala` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "  implicit def augmentString(x: String): StringOps = \n",
    "      new StringOps(x)\n",
    "*/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a conditional implicit value, because the argument `x` is not implicit. This is a so-called *implicit conversion* that allows the Scala compiler to convert a `String` into an object of type `StringOps`. This class is the one that actually provides the new method `map`. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mMyStringOps\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyStringOps(x: String){\n",
    "    def mymap(f: Char => Char): String = \n",
    "        \"dummy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in order to extend the `String` class with this new method, we have to do something similar to what the Scala library does, i.e. provide an implicit conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36mtoMyStringOps\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit def toMyStringOps(x: String): MyStringOps = \n",
    "    new MyStringOps(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres26\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"dummy\"\u001b[39m"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hiii\".mymap(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pattern is so common that Scala 2.10 provided a special construct for it: *implicit classes*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mclass\u001b[39m \u001b[36mMyStringOps2\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "implicit class MyStringOps2(x: String){\n",
    "    def mymap2(f: Char => Char): String = \n",
    "        \"dummy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mres32\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"dummy\"\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"oooo\".mymap2(identity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits for ad-hoc polymorphism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordering[T]\n",
    "Encoder[T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implicits for type-level computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
